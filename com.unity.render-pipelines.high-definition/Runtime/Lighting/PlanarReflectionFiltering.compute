#pragma kernel FilterPlanarReflection
#pragma kernel DownScaleReflection

#pragma only_renderers d3d11
// #pragma enable_d3d11_debug_symbols

// HDRP generic includes
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/ShaderLibrary/ShaderVariables.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/Material/Material.hlsl"

// Tile size of this compute
#define PLANAR_REFLECTION_TILE_SIZE 8

// This texture holds a radian value that matches the angle to grab 70% of the energy of the ggx lobe.
TEXTURE2D(_ThetaValuesTexture);
// Used to sample the theta values texture
SAMPLER(s_linear_clamp);
// Mip chain of depth and color
TEXTURE2D(_DepthTextureMipChain);
TEXTURE2D(_ReflectionColorMipChain);

CBUFFER_START(ShaderVariablesPlanarReflectionFiltering)
    // The screen size (width, height, 1.0 / width, 1.0 / height) that is produced by the capture
    float4 _CaptureBaseScreenSize;
    // The screen size (width, height, 1.0 / width, 1.0 / height) of the current level processed
    float4 _CaptureCurrentScreenSize;
    // Normal of the planar reflection plane
    float3 _ReflectionPlaneNormal;
    // Roughness value of the current integration
    float _IntegrationRoughness;
    // World space position of the planar reflection (non camera relative)
    float3 _ReflectionPlanePosition;
    // FOV of the capture camera
    float _CaptureCameraFOV;
    // World space position of the capture camera (non camera relative)
    float3 _CaptureCameraPositon;
    // The mip index of the source data
    int _SourceMipIndex;
    // Inverse view projection of the capture camera
    float4x4 _CaptureCameraIVP;
    float _RTScaleFactor;
CBUFFER_END

// Output buffer of our code
RW_TEXTURE2D(float4, _FilteredPlanarReflectionBuffer);

bool IntersectPlane(float3 ray_origin, float3 ray_dir, float3 pos, float3 normal, out float t)
{
    t = -1.0;
    float denom = dot(normal, ray_dir); 
    if (abs(denom) > 1e-5)
    { 
        float3 d = pos - ray_origin;
        t = dot(d, normal) / denom;
        return (t >= 0); 
    }
    return false; 
}

float sqr(float value)
{
    return value * value;
}
float gaussian(float radius, float sigma)
{
    return exp(-sqr(radius / sigma));
}

[numthreads(PLANAR_REFLECTION_TILE_SIZE, PLANAR_REFLECTION_TILE_SIZE, 1)]
void FilterPlanarReflection(uint3 dispatchThreadId : SV_DispatchThreadID, uint2 groupThreadId : SV_GroupThreadID, uint2 groupId : SV_GroupID)
{
    UNITY_XR_ASSIGN_VIEW_INDEX(dispatchThreadId.z);

    // Compute the pixel position to process
    int2 currentCoord = (int2)(groupId * PLANAR_REFLECTION_TILE_SIZE + groupThreadId);

    // Compute the coordinates that shall be used for sampling
    float2 sampleCoords = (currentCoord << (int)(_SourceMipIndex)) * _CaptureBaseScreenSize.zw * _RTScaleFactor;

    // Read the a filtered depth value of the tap coord
    float centerDepthValue = SAMPLE_TEXTURE2D_LOD(_DepthTextureMipChain, s_trilinear_clamp_sampler, sampleCoords, _SourceMipIndex).x;

    // Compute the world position of the tapped pixel
    PositionInputs centralPosInput = GetPositionInput(currentCoord, _CaptureCurrentScreenSize.zw, centerDepthValue, _CaptureCameraIVP, 0, 0);
    
    // Compute the direction to the reflection pixel
    float3 rayDirection = normalize(centralPosInput.positionWS - _CaptureCameraPositon);

    // Compute the position on the plane we shall be integrating from
    float t = -1.0;
    if (!IntersectPlane(_CaptureCameraPositon, rayDirection, _ReflectionPlanePosition, _ReflectionPlaneNormal, t))
    { 
        // If there is no plane intersection, there is nothing to filter (means that is a position that cannot be reflected)
        _FilteredPlanarReflectionBuffer[currentCoord] = float4(0.0, 0.0, 0.0, 1.0);
        return;
    }

    // Compute the integration position (position on the plane)
    float3 integrationPositionRWS = _CaptureCameraPositon + rayDirection * t;

    // Evaluate the cone halfangle for the filtering (it is the cone angle to grab 70% of the energy of the ggx lobe)
    float halfAngle = SAMPLE_TEXTURE2D_LOD(_ThetaValuesTexture, s_linear_clamp, float2(dot(rayDirection,_ReflectionPlaneNormal), _IntegrationRoughness), 0).r;
    halfAngle = halfAngle * PI * 0.5f;

    // Compute the distances we need for our filtering
    const float distanceCameraToPlane = length(integrationPositionRWS - _CaptureCameraPositon);
    const float distancePlaneToObject = length(centralPosInput.positionWS - integrationPositionRWS);

    // Compute the cone footprint on the image reflection plane for this configuration
    const float brdfConeRadius = tan(halfAngle) * distancePlaneToObject;

    // We need to compute the view cone radius
    const float viewConeRadius = brdfConeRadius * distanceCameraToPlane / (distancePlaneToObject + distanceCameraToPlane);

    // Compute the view cone's half angle. This matches the FOV angle to see exactly the half of the cone
    const float viewConeHalfAngle = atan(viewConeRadius / distanceCameraToPlane);
    // Given the camera's fov and pixel resolution convert the viewConeHalfAngle to a number of pixels
    const float pixelDistance = viewConeHalfAngle / _CaptureCameraFOV * _CaptureCurrentScreenSize.x;
    // Convert this to a mip level shift starting from the mip 0
    const float miplevel = log2(pixelDistance / 2);

    // Read the integration color that we should take
    const float3 integrationColor = SAMPLE_TEXTURE2D_LOD(_ReflectionColorMipChain, s_trilinear_clamp_sampler, sampleCoords, miplevel + _SourceMipIndex).xyz; 

    // Write the output ray data
    _FilteredPlanarReflectionBuffer[currentCoord] = float4(integrationColor, 1.0);
}

// Half resolution output texture for our mip chain build.
RW_TEXTURE2D(float4, _HalfResReflectionBuffer);
RW_TEXTURE2D(float, _HalfResDepthBuffer);
// Given that the camera is using an oblique projection matrix, the inversion code doesnt work properly
// If we detect that we are on the far plane, we override the value by an other one. This value has been choosen experimentally.
#define OBLIQUE_CAMERA_DEPTH_CLAMP_VALUE 0.75

[numthreads(PLANAR_REFLECTION_TILE_SIZE, PLANAR_REFLECTION_TILE_SIZE, 1)]
void DownScaleReflection(uint3 dispatchThreadId : SV_DispatchThreadID, uint2 groupThreadId : SV_GroupThreadID, uint2 groupId : SV_GroupID)
{
    UNITY_XR_ASSIGN_VIEW_INDEX(dispatchThreadId.z);

    // Compute the pixel position to process
    int2 currentCoord = (int2)(groupId * PLANAR_REFLECTION_TILE_SIZE + groupThreadId);

    // Here we have to go wider than the simple 2x2 neighborhood or there is too much aliasing
    float3 averageColor = 0.0;
    float sumW = 0.0;
    const float radius = 2;
    const float sigma = radius * 0.75;
    for (int y = -radius; y <= radius; ++y)
    {
        for (int x = -radius; x <= radius; ++x)
        {
            const int2 tapCoord = currentCoord * 2 + uint2(x, y);
            float r = sqrt(x*x + y*y);
            // If the pixel is outside the current screen size, its weight becomes zero
            float weight =  tapCoord.x > _CaptureCurrentScreenSize.x ||  tapCoord.x < 0 
                            || tapCoord.y > _CaptureCurrentScreenSize.y ||  tapCoord.y < 0 ? 0.0 : gaussian(r, sigma);
            averageColor += LOAD_TEXTURE2D_LOD(_ReflectionColorMipChain, tapCoord, _SourceMipIndex) * weight;
            sumW += weight;
        }
    }
    // Normalize and output
    _HalfResReflectionBuffer[currentCoord] = float4(averageColor / sumW, 1.0);

    // Read the depth values
    float d0 = LOAD_TEXTURE2D_LOD(_DepthTextureMipChain, currentCoord * 2, _SourceMipIndex);
    float d1 = LOAD_TEXTURE2D_LOD(_DepthTextureMipChain, currentCoord * 2 + int2(1, 1), _SourceMipIndex);
    float d2 = LOAD_TEXTURE2D_LOD(_DepthTextureMipChain, currentCoord * 2 + int2(0, 1), _SourceMipIndex);
    float d3 = LOAD_TEXTURE2D_LOD(_DepthTextureMipChain, currentCoord * 2 + int2(1, 0), _SourceMipIndex);

    // We we are on the far plane override the value
    d0 = d0 == 0.0 ? OBLIQUE_CAMERA_DEPTH_CLAMP_VALUE : d0;
    d1 = d1 == 0.0 ? OBLIQUE_CAMERA_DEPTH_CLAMP_VALUE : d1;
    d2 = d2 == 0.0 ? OBLIQUE_CAMERA_DEPTH_CLAMP_VALUE : d2;
    d3 = d3 == 0.0 ? OBLIQUE_CAMERA_DEPTH_CLAMP_VALUE : d3;

    // Average and output to the depth buffer
    _HalfResDepthBuffer[currentCoord] = (d0 + d1 + d2 + d3) * 0.25f;
}